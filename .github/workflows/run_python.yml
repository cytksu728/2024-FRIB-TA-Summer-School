
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3a6eca-0be8-453c-b7e8-cb090da5e02b",
   "metadata": {},
   "source": [
    "# In this notebook we will calibrate a linear model using the Bayesian Approach\n",
    "\n",
    "Thanks to ChatGPT for helping in the creation and comments of the code!\n",
    "\n",
    "Thanks to Pablo for correcting ChatGPT on the few mistakes it made and for writing more comments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ffb11-3901-4bca-82cc-0ec42b4fb967",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f75145-898b-493e-8e49-d8e723e8538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(142857)\n",
    "\n",
    "# Generate 20 points normally distributed around a straight line y = 2x + 1\n",
    "x = np.linspace(0, 5, 20)\n",
    "\n",
    "# Noise level\n",
    "noise_size = 1\n",
    "noise = np.random.normal(0, noise_size, size=x.shape)\n",
    "\n",
    "def true_f(x,a0=1,a1=2):\n",
    "    return x*a1 + a0\n",
    "\n",
    "def model_f(x,a0,a1):\n",
    "    return x*a1 + a0\n",
    "#Seems silly that we are basically defining the same function twice, but this way we can play with\n",
    "# a different generating function for the true values later if we wanted to\n",
    "\n",
    "y = true_f(x) + noise\n",
    "\n",
    "\n",
    "# Generating function\n",
    "y_true = true_f(x)\n",
    "\n",
    "# Plot the points and the generating function\n",
    "plt.figure(figsize=(6, 4),dpi=150)\n",
    "plt.plot(x, y_true, label='True function: $y = 2x + 1$', color='blue')\n",
    "plt.errorbar(x, y, yerr=noise_size, fmt='o',markersize=4, color='red', label='Noisy data points', ecolor='black', elinewidth=1.5, capsize=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ffa99f-40e0-4959-a079-a6766f2739f4",
   "metadata": {},
   "source": [
    "## Bayesian calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259516c7-4fba-4ca4-9e09-dc37c3e92f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian calibration using MCMC with Metropolis-Hastings algorithm\n",
    "\n",
    "#Since likelihoods and priors usually look like exponentials (e^{stuff}), it is customary to work with their likelihoods.\n",
    "#This makes it easier when handeling very tiny numbers, and they can be exponentiated later to build the posterior when doing the metrolopis algorithm\n",
    "\n",
    "\n",
    "#Assuming a gaussian likelihood, what sits on top of it its what we usually call \"chi2\". We are also\n",
    "# assuming that we know the error bars of the points perfectly well and that is \"sigma\", but if\n",
    "# we didn't know it, there are ways of estimating it at the same time:\n",
    "\n",
    "def log_likelihood(a0, a1, x, y, sigma=1):\n",
    "    y_model = a0 + a1 * x\n",
    "    return ... what goes here?\n",
    "\n",
    "\n",
    "\n",
    "#We have some prior knowledge of around where these things should be\n",
    "a0_mu_prior=\n",
    "a1_mu_prior=\n",
    "\n",
    "#How sure are we about that? Bigger sigmas here mean we are less sure:\n",
    "a0_sigma_prior=\n",
    "a1_sigma_prior=\n",
    "\n",
    "def log_prior(a0, a1):\n",
    "    return ... what goes here?\n",
    "\n",
    "def log_posterior(a0, a1, x, y, sigma=1):\n",
    "    return log_prior(a0, a1) + log_likelihood(a0, a1, x, y, sigma)\n",
    "\n",
    "# Initial guess\n",
    "a0, a1 = 1, 1\n",
    "\n",
    "\n",
    "n_iterations = 100000\n",
    "\n",
    "\n",
    "#This step controlls how much we move in the parameter step on each MCMC iteration\n",
    "step = 0.2\n",
    "\n",
    "# Storage for samples\n",
    "samples = np.zeros((n_iterations, 2))\n",
    "samples[0, :] = [a0, a1]\n",
    "\n",
    "# Count the number of acceptances\n",
    "n_acceptances = 0\n",
    "\n",
    "for i in range(1, n_iterations):\n",
    "    a0_proposal = a0 + np.random.normal(0, step)\n",
    "    a1_proposal = a1 + np.random.normal(0, step)\n",
    "    \n",
    "    log_posterior_current = log_posterior(a0, a1, x, y, sigma=1)\n",
    "    \n",
    "    log_posterior_proposal = ... what goes here?\n",
    "    \n",
    "    acceptance_prob = ... what goes here?\n",
    "    \n",
    "    if np.random.rand() < acceptance_prob:\n",
    "        what do we do if the aceptance probability is bigger?\n",
    "        n_acceptances += 1\n",
    "    \n",
    "    samples[i, :] = [a0, a1]\n",
    "\n",
    "# Calculate and print the acceptance ratio\n",
    "acceptance_ratio = n_acceptances / n_iterations\n",
    "print(f'Acceptance ratio (should be around 1/3): {acceptance_ratio:.2f}')\n",
    "\n",
    "# Plotting the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(samples[:, 0], label='a0')\n",
    "plt.plot(samples[:, 1], label='a1')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Parameter value')\n",
    "plt.legend()\n",
    "plt.title('MCMC Samples for a0 and a1')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a0c1d-b3dc-4e9c-be7e-ba66104d93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is a fake list of points just to show you how to plot them in the corner plot package. Replace by your own \n",
    "samples_fake= np.column_stack((np.random.normal(-10, 1, 100000), np.random.normal(-15, 2, 100000)))\n",
    "\n",
    "\n",
    "# Determine the limits for the corner plot\n",
    "a0_min, a0_max = np.percentile(samples_fake[:, 0], [1, 99])*(0.4,1.2)\n",
    "a1_min, a1_max = np.percentile(samples_fake[:, 1], [1, 99])*(0.9,1.1)\n",
    "\n",
    "corner.corner(samples_fake, labels=[\"a0\", \"a1\"], truths=[1, 2],truth_color='r', \n",
    "              # range=[(a0_min, a0_max), (a1_min, a1_max)], #Uncomment this line for the real case\n",
    "              bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd32dc-b2b2-4d31-b348-46586a083a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to calculate the posterior distribution of the model lines themselves:\n",
    "\n",
    "\n",
    "# Sampling 10,000 curves from the visited samples\n",
    "n_samples = 10000\n",
    "indices = np.random.choice(range(n_iterations), n_samples, replace=True)\n",
    "\n",
    "sampled_a0 = what goes here?\n",
    "sampled_a1 = what goes here?\n",
    "\n",
    "x_extended = np.linspace(-2, 8, 100)\n",
    "y_true_extended=true_f(x_extended)\n",
    "\n",
    "\n",
    "# Generate y values for each sampled parameter set\n",
    "y_samples = what goes here?\n",
    "\n",
    "# Calculate mean and credible intervals for each x\n",
    "y_mean = np.mean(y_samples, axis=0)\n",
    "y_lower = np.percentile(y_samples, 2.5, axis=0)\n",
    "y_upper = np.percentile(y_samples, 97.5, axis=0)\n",
    "\n",
    "# Plotting the estimated mean model and credible intervals\n",
    "plt.figure(figsize=(6, 4),dpi=150)\n",
    "plt.plot(x_extended, y_true_extended, label='True function: $y = 2x + 1$', color='blue')\n",
    "plt.errorbar(x, y, yerr=noise_size, fmt='o',markersize=4, color='red', label='Noisy data points', ecolor='black', elinewidth=1.5, capsize=3)\n",
    "plt.plot(x_extended, y_mean, label='Mean model', color='green')\n",
    "plt.fill_between(x_extended, y_lower, y_upper, color='green', alpha=0.3, label='95% Credible interval')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fbcec-909a-4485-882a-33e2fcdfb5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
